% .MDSH IS MARKDOWN +X -> http://freeze.sh/2015/mdsh
%
% lines starting with % are comments and therefore will be ignored
% BUT: A COMMENT MAY BE FUNCTIONAL (= interpreted during export)
% A text [^]{a footnote} and some more text
% A citation reference [@fsf:2007:gpl]
% A citation reference with pagenumber [@[1-8]kay:1990:uiapv]
% -> bibtex references: http://freeze.sh/etherpad/bibref
% -> markdown reference: http://daringfireball.net/projects/markdown

% --------------------------------------------------------------------------- %


% TITLE: The Shrink-Wrapped Design Process
%        ---------------------------------
% BY: Michael van Schaik
% LICENSE: GPL/GFDL/FAL

% http://forkable.eu/utils/texts/goto10.org/FLOSS+Art.v1.1.eBook-GOTO10/
% http://ospublish.constantvzw.org/documents/FLOSS+Art.pdf


Preface
=======

It takes us years of education to learn to read, write and talk as ways
of interacting with our environment. Isn't it strange, then, that one
receives barely any kind of education in interacting with computers (as
a modern communication environment)? Instead most computer programs are
built to use interfaces as translators from computer to human. Although
this is meant to help, in many cases this also makes users less powerful
because they can only consume set functions within the program. You can
only compute what the software lets you compute, and only manipulate
what you see.

This is especially the case in the world of graphic design; due to the
global monopoly of big firms like Adobe (which recently even bought it's
only real competitor Macromedia), all graphic designers seem forced to
use the same software to produce their work. Strangely enough, the
lion's share of designers don't really seems to care. Looking at my own
education (in the Netherlands), a reason for this could be that the
discourse here seems to be focused on ideas more than actual products
(and the tools used to create them). Though there seems to be a growing
interest in alternatives such as open source software as a means to a
more independent design practice, the problem with these is that the few
packages that are currently the killer apps in this field are often just
free-er copies of existing software, not much different from their
proprietary counterparts.

What if designers actually learned to use the real power of the computer
and computational media instead of the ready-made physical workspace
that it currently tries to represent? There is an interesting
opportunity here for designers to start using the computer as the
universal machine that it really is instead of a shrink-wrapped special
purpose machine modelled on analogue tools, locking them away from new
creative possibilities specific to the field of computational media. Why
learn to use only the Adobe tool-chain if one can build one's own
production-environment tool by tool?


Current Situation
=================

The main software tool-chains used in the field of graphic design can be
divided in to "digital" and "print" oriented. Since most classically
educated graphic designers (like myself) are primarily educated in the
field of print production, it shouldn't come as a surprise that this is
also the main focus of most software. The main, widely used
print-production software can be classified under three categories:
vector based (Illustrator, Freehand), bitmap based (Photoshop,
Fireworks) and software to gather and combine the two for prepress
(X-press, inDesign). The software that focuses more on digital-design
such as design for websites and CD-roms/DVDs generally tries to
represent these media in a way designers are already familiar with from
the world of print. What these software programs have in common is that
they are an interface between a technical system and the visual
workspace of a designer. So placing an image on a website works in just
the same way as placing it in a book; the designer just imports the
image and it's there. What actually happens beneath the surface of the
Graphical User Interface stays hidden. This might be considered a good
thing, since it saves the designer from having to learn a whole new way
of working for any new technical system, it's just a matter of learning
it once and then using the same techniques throughout all media designs.
And who wants to know what's "under the hood" anyway? -- that's the world
of programmers, which is generally not of much interest to graphic
designers.

Without advocating Friedrich Kittler's suggestion that computer users
learn to read the true (hexadecimal) language of the computer, hiding
the real (raw) material of a medium from the user is indeed an important
paradigm common in most software engineering.[@kittler:1995:tins] It
can be considered the reason for the sort of fear of programming code
that seems to exist among designers (and lots of computer users in
general). Code is equal to problems. Code is where you see the actual
computer break through the graphical skin, like in the art of JODI, or
the field of glitch art, which is about errors and other things you'd
like to prevent.[@wikipedia:2010:jodi] Take a look at, for example,
the Macintosh OS X operating system, where the "engine" of the computer
is almost invisible for the common user. You just do whatever you want
to do as if you were in the physical world, and, invisibly, the system
takes care of the technical layer. One never sees any program code, or
even a glimpse of how the system is built up, while the fact that this
system is based on Unix is considered one of its main advantages (The
Unix-philosophy typically means having simple applications doing their
job as well as possible, instead of wasting CPU power on things like
dynamically resizing screens and using a "genie effect" on hiding them).

The fear of program-code may originate from the first desktop systems,
which introduced this shrink-wrapping of program code in a nice GUI
(Graphical User Interface). And this fear is also very much there among
graphic designers. It is considered their job to work in the visual
field, the field of texts, nice fonts and images, and the computer is
supposed to generate the appropriate code, say postscript (for print) or
html or actionscript etc. (for digital media). This way the designer has
the feeling of staying "above" the medium, calibrating it to their
needs, unaffected by its actual individual (technical) capabilities and
way of working. However, if we look at the historical context designers
have worked in, things weren't always like this. On the contrary, well
known designers such as Piet Zwart or Wim Crouwel, for example, were
actually working with new techniques (photography and photographic
typesetting) and challenging, changing and creating the possibilities of
these techniques. Through their knowledge of these new media, they also
learned their intrinsic qualities. Thus, photography introduced the
photographic image into design, but instead of taking that for granted,
Piet Zwart re-appropriated these images as design elements. And while
photographic typesetting was invented to easily reproduce existing type,
Crouwel instead invented type specifically for the capabilities and
restrictions of this technique. These are things that wouldn't have been
possible without a thorough (technical) knowledge of the medium.

So, taking fear of the actual medium of the computer, program code, as a
given, it is interesting to look at how this fear in fact restrains the
designer nowadays. As mentioned, artistic education is focused mainly on
the development of the self, and of original ideas. There is less
interest in or time for studying the digital media used to communicate
them. Because of the fear of being restrained by (the absence of
knowledge of) program code, instead of the freedom this knowledge might
give, the focus (at least in the Netherlands) is much more on concepts
and physical or visual techniques. But what is forgotten is the
importance of the medium in constituting part of the message. The
computer is therefore, mistakenly, seen as just a meta-medium. One that
reproduces all earlier media without changing them. Think also here of
how metaphors like the paper folder in the desktop or the term "web
pages" in the internet are used. And this last example is also one of
the more obvious illustrations of the constraint this disinterest
in/fear of the technical produces. The internet is, obviously, a totally
different medium from print but by applying this metaphor of a page as a
piece of paper, designers become its captive, in turn producing ideas
such as websites that one can flip through like pages in a book (next,
previous). This places a huge restriction on the actual "hyper-"quality
of the web.

In order to understand the media we work in, there should be much more
focus on learning and understanding at least the basics of their
intrinsic technical layers. This way designers may acquire more of an
insight into the constraints GUI programs impose on the way they design
our messages/media.


FLOSS vs. Proprietary, or: Why it is Important to Study Our Software
====================================================================

The importance of knowledge for fully understanding and using or even
designing the medium may seem quite obvious. It is also important,
however, for less obvious reasons. If we consider graphic design a form
of communication, or even a cultural activity, it is highly important
for it to be totally free. Free to question, challenge and form critical
opinions, free to question authority, free to protest and free to
question and shape the media it works with. Therefore, knowledge of
technology should also be considered an important way to gain insight
into how political choices in software may invisibly inhibit a truly
free and critical stance. To give an example, according to Microsoft the
new Windows Vista operating system

> introduces a breakthrough user experience and is designed to help you
> feel confident in your ability to view, find, and organize information
> and to control your computing experience.
> [@microsoft:2010:vista]
% http://www.microsoft.com/uk/windows/products/windowsvista/features/default.mspx 

The last part of this claim in particular can be seen in a totally
different light if combined with the claim the Free Software Foundation
makes on badvista.org:

> Among other harms, BadVista.org will focus on the danger posed by
> Treacherous Computing in Vista. Commonly called Trusted Computing in
> the industry, it is an attempt to turn computers from machines
> controlled by their user into machines that monitor their user and
> refuse to operate in ways that manufacturers don't authorize.
> [@fsf:2010:vista]
% http://badvista.fsf.org/

Though the last claim (as well as the first) should be considered an
opinion, their combination gives a nice insight into how at least some
technical knowledge should be considered a necessity to gain insight
into the political choices forming the bases of software. Another way
software can constrain its users may be found in the example of big
software companies "donating" computers to schools or institutes and
thereby implicitly forcing them to learn, use and thus buy their
specific products. Or as Jaromil put it: "teaching them to use Sony
cameras."[^]{in personal conversation with Jaromil, November 2006}
The strange thing is that while there exist excellent alternatives
to most proprietary software in the form of Open Source software,
this doesn't seem to catch on very well. Open Source software is 
free (libre) software because the source code of the software is
publicly available (or available after one has purchased the software).
It is often free (gratis), created and maintained by networked
communities of hackers and programmers. The importance of this software
lies in the fact that it makes the user free to study or change it
without being dependent on the original creators. Another important
paradigm in this field is Open Standards, meaning the file formats used
are documented and can be freely studied and implemented. An example of
Open Standards can be found in the Open Document Format, which
originates from the xml-based file format used by Open Office. Open
Office is nowadays a fully working, Open Source alternative for closed
proprietary software suites like Microsoft Office. The Open Document
Format is a freely implementable file format that, because of its Open
Standards, can be studied without restrictions. The value of the
transparency and freedom that comes with these kinds of formats is
underlined by the decision of the Dutch government to fully adopt open
source software and open standards by 2006 (the "Motie Vendrik",
accepted in 2002). In June 2006 the Belgian government even decided to
prohibit closed formats in official government documents by 2008.

Though change takes time, this is also caused by the historic policy of
deterring uptake of Open Source and Open Standards by those companies
currently holding the biggest market share. Much of the history of free
software is about circumventing restrictions put in place by companies.
For example, at the time of writing Microsoft Office still does not
support Open Document Format, which may be an important reason why the
Dutch government's progressive plans decided back in 2002 have yet to be
realised.

And if, for example, Adobe, nowadays the publisher of Flash and the
Flash-player, were to implement support for the (open standard) Scalable
Vector Graphics format, the viewer-coverage for this Open Standard file
format would instantly be about 100 percent. Instead they have decided
to cease development of their svg plugin which was one of the few ways
people using Internet Explorer were able to view svg content (since IE,
surprisingly, is the only browser which hasn't got any sort of basic
native svg support). Luckily most other browsers have some form of
native support for svg graphics built in, it's just too bad that about
70 percent of computer users are still using Internet Explorer.

The same story, by the way, goes for support of the (24-bit) png format
(indeed, also an open standard), which all browsers except IE support
natively. With the release of IE7 this seemed to have improved, except
that this version is built not to install on systems older than Windows
XP which, in turn, will not run on older hardware.


Software is the Message
=======================

To get an insight into how software shapes our message, it might be nice
to take a look at linguistics, for here the influence of the computer is
quite remarkable. "Browsing" as a word for finding information (as in a
library), "system crash" for not being able to remember something or
even "ctrl-z" for undoing a mistake. These terms have not only become
part of the wider language, their influence reaches far further. The
software we use determines how we think. I personally noticed this
phenomenon when I once tried to use sticker-remover to remove some
traces of tape on a coffee-machine. The tape dissolved just fine, but so
did the plastic of the coffee-machine. The first thing that popped into
my mind was to press Cmd-z (I "Think different").

According to Marshall McLuhan,

> The new medium of TV as an environment creates new occupations. As an
> environment, it is imperceptible except in terms of its content. That
> is, all that is seen or noticed is the old environment, the movie. But
> even the effects of the TV environment in altering the entire
> character of human sensibility and sensory ratios is completely
> ignored. The content of any system or organization naturally consists
> of the preceding system or organization, and in that degree acts as a
> control on the new environment. It is useful to notice all of the arts
> and sciences as acting in the role of anti-environments that enable us
> to perceive the environment.[@mcluhan:2005:mmu]

However, according to media theorist Arjen Mulder, while computers are
indeed often said to also remediate the preceding media, they actually
are a "metamedium", a combination of all previous media:

> Only in this case, it is not so much a matter of remediation of all
> old and new media, as it is a hybridization, a melting of unequal
> media to a new unity.[@mulder:2002:om]

Another example, which a friend with a background in architectural
studies recently pointed out to me, is that with the introduction of
each new software used in architecture, a new trend is born. For
example, the release of Maya 3D sparked blob-architecture, whose shapes
were specific to this software and could easily be traced back to the
lines of software code from which they were generated.

This leads to another interesting question: who is the producer and who
is the consumer in this software setup? In a way the software companies
are the producers of the environment in which certain ideas develop. One
could argue this by saying software is just a tool in the process of
developing and communicating ideas, but in the case of Maya, the
generative algorithm is created by a software developer. Isn't it true
that the "designer" is actualising a potential form generated by the
software developer's algorithm?

Of course this comparison might be seen as an exaggeration, but it
illustrates the basic point very well. What a user/consumer of software
is actually doing is placing his ideas in a preset environment. This
works both ways, so (unconsciously) he is also conforming his ideas to
this set environment. If we look at the example of Maya, the ideas of
blob-architecture might well be developed according to a certain
"zeitgeist", but by elaborating them with this software the resulting
physical shape becomes far from universal and is, as mentioned,
sometimes very specific to the software used. In this instance we might
even wonder who the actual producer of the blob-building is.

Software always (in a certain way) influences your thinking. Analogous
to "the medium is the message", the software has become the medium
(and thus, the message).


Software Filters the Message
============================

With the invention of the private computer, its developers started
searching for ways to make it understandable to non-scientists. Maybe
the most important development in this field has been the desktop
metaphor. So far it has seemed to work fairly well; developers started
to put all kinds of these metaphors into software and they made people
understand computers better because of the references to existing
physical objects. But the problem is that this imposes the idea of the
computer as meta-medium. As a result the computer as medium in itself
has become invisible behind all kinds of brush-metal windows and 3D-push
buttons, and we tend to forget to question their logic. We often take
software as a given fact, something that's just there.

But software, apart from influencing messages because of it's logic, has
another layer of influence, which is now becoming more obvious every
day; politics. If we communicate using a software structure like the
internet, aspects of the wider world such as free speech become an
important issue here, too. That's why it is so striking to see software
companies functioning as an extension of less free regimes, such as
Google for the government of China. Here Google is developing ways to
filter the content of the "free" web, so that its Chinese users will not
find results that are considered harmful by the Chinese government. (It
should be mentioned here that Google was one of the last search engines
to submit to these demands from the Chinese government, which is both
admirable and an index of the companies'' power).

Without necessarily passing judgement on Google or the Chinese
government in this situation, it is a good example of why it is
important for us to be able to look "through" software, to study the
environments that manipulate our thoughts and that manipulate the
information that is at the basis of formulating these thoughts. The
inner-workings of Google are a big secret and can therefore in no way be
objectively monitored. Hypothetically, what would prevent them from
filtering results on, say, a person-to-person basis, or on an
ip-location basis? These examples might seem far-fetched, but if I
search using the term "test" on google.nl I get different and fewer
results using the Safari browser than if I use Firefox (806 million
results in Safari vs. 843 million in Firefox). This can be interpreted
as at least some sort of browser-specific filtering already going on.
It's also no big secret that companies offering gratis services on the
internet often generate money by selling the data they gather through
their services.

A friend gave me another recent example of software filtering of the
message that is fuelled by politics: part of his practice as an artist
involves researching new media. Another part involves photography,
occasionally containing artistic nudity. So when a new system like
Flickr appeared, he became an early and extensive user. It offered a
very interesting way of combining two of his interests. But when Flickr
was acquired by Yahoo and so became subject to its rules he was asked to
remove his photos because they were considered sexually explicit
content. Though internet companies haven't at present much to do with
the software we use to create graphic design, the situation is a strong
example of the problem of relying completely on proprietary software. We
have no way to monitor how they work, nor are we able to check what the
major software companies might be planning in the future for their
services. Some nowadays repressive regimes started out as _Don't be
evil_ revolutions (e.g. China) but power seems to have strange effects
on people.[^]{Google's corporate philosophy [@google:2010:gcoc]}
We should never blindly rely on a particular software as our main let
alone our only platform.


OS / Free Software vs. Free Design / Libre
==========================================

In the field of graphic design Open Source development can be regarded
as a healthy alternative to proprietary software because it offers the
possibility of studying / changing the source code, thereby disclosing
how the program works, and how it may influence its user. This makes
users less dependent on the original developer to make changes or solve
bugs. Also, because most open source software uses Open Standards for
saving files, users will not likely _buy in_ to a specific software but
are free to exchange files and projects between different applications.
At least, that is the idea of Open Standards.

These are interesting times in the development of Open Source
possibilities for graphic designers. For quite a lot of proprietary
software a fairly developed open source alternative can be found. In the
case of inDesign that would be Scribus, for Photoshop, The Gimp and for
Illustrator, Inkscape. Then there are a whole bunch of less familiar but
equally interesting software developments. For the purpose of giving an
overview of the basic design software, however, these three examples
should suffice.

Without wanting to deny the political and philosophical impact of these
projects, a practical issue is that they actually aren't much more than
a "free/libre" clone of existing proprietary software. The reason for
this is simple, as Craig Bradney, part of the Scribus development team
says:

> Our aim is not to replace wholesale other applications, but to provide
> the best DTP experience for Linux users -- to provide the same kinds of
> tools users have had on other platforms and to use the inherent
> strengths of "Open Source Software and Linux/Unix" to Scribus
> advantage. [@fosdem:2006:scribus]

Though this is a perfectly legitimate reason for developers to create
their software, the problem is twofold: firstly, the focus on "free as
in libre", is easily displaced to "free as in beer". What is the
importance of these developments, beside the fact that it doesn't cost
money to use them? Of course there is the advantages of it undermining
the monopolistic landscape of (design) software, and that users are not
made dependent on big companies. Although most Linux users are aware of
this, and it may be a key consideration when they choose whether or not
to use a particular piece of software, I have my doubts about how
important this aspect really is to other users. Running Open Office on
Windows is a lot less about being "free" and probably a lot more about
being "gratis". Though that's as good a reason as any, this can in some
ways be compared to using a black market reproduction of a product that
has taken years of research and development. It looks the same, it works
the same, but it's Open Source. This comparison opens up another
discussion in which one could argue that the way big companies have been
able to become this big and rich is by underpaying their workers and
patenting (other) peoples'' ideas. Here, a motive could also be found
for pleading for the right to take this knowledge back and make it
public property. This is, however, something which will probably remain
a point of contention for a long time, and is certainly not something I
want to pass judgement on here.

The second problem, which is actually more important to me, is that as a
clone of existing software, these open source alternatives also copy the
traps and mental models of their properietary counterparts. Therefore,
in my opinion, for "free software" to be a valid term, the open sourcing
of closed software by reverse-engineering should be seen as only a
beginning; a way to make software available as a field of study in order
to discover its inner logic; to redevelop it in order to be really "free
as in free speech", or even thought. This doesn't seem to be the aim of
the free software movement at the moment. Just as much as the
proprietary software it resembles, it focuses on making computing
understandable by hiding it behind metaphors. These metaphors might
actually be where the real problem lies.

If we take a look, for example, at Pure Data, a software developed as an
open source project intended to work with any data, we see that there is
much less hiding going on. The data is "wired" around by simple lines
(like a pipe in the Unix command line environment), resembling nothing
other than a line. Data flowing around can be manipulated in any
conceivable way, if it don't exist yet, it can be programmed. This same
kind of openness or freedom can also be found in another open source
project: Processing. As the website explains: Processing is an open
source programming language and environment for people who want to
program images, animation, and sound. [...] It is created to teach
fundamentals of computer programming within a visual context and to
serve as a software sketchbook and professional production
tool.[@processing:2006:site]

This description also suggests a shift that seems necessary in current
design education. In order to develop the design profession further in
an interesting way instead of the constant spiral of visual sampling
that it is now, a better understanding of media should be developed. Now
that everybody can use software, and the pragmatic (theoretical)
technical skills can be coded into computer systems, it becomes more and
more important to develop this software further. Because, if software is
the medium, it will be this software (digital media) that is part of the
future subject of design.

I'm not trying to say here that the formal study of conceptual thinking,
or theoretical and technical knowledge, will become obsolete. Merely
that, by getting a better understanding of our media, we will be able to
design them better and to develop the design field further. To enable
this understanding, it will be necessary to stop superimposing physical
metaphors on everything computer related. This, as I have said, is
causing an incorrect understanding of our media. This is the reason why,
when constructing a website, designers often fail to use the potential
of hypertext and hyper structures. They are trained to create an all
inclusive conceptual idea and to communicate that within the technical
possibilities of the medium (print). This might be why a website is
often called a "page" and why designers often design them as if they
were printed (preferably including kerning and all kinds of custom
typefaces). In the digital media this attitude can only be seen as a
dictatorial obsession, because part of (the strength of) digital media
such as hypertext is that it can look different on any computer. Users
get a say in how they would like to use the information you have
designed, and therefore the design should focus more on structure and
route than on punctuation. This obsessive focus on detail might be
inherited from the physical world whose design and printing processes
have been reproduced in computers. New media are much more modular.

With the emerging generations of designers having been surrounded by
computers all their life, it's time to step out of representing the
physical media within the computer. Designers should be trained in (at
least the basics of) the underlying codes that make up the digital media
in order to be able to really understand and use them. As Florian Cramer
states:

> The closer the software is to the daily needs and work methods of
> programmers and system administrators, the higher typically its
> quality.[@cramer:2006:ccmisunderstanding]

This shows how important it is for designers to start taking a more
critical stance toward their software and to study it. The best Open
Source software has been written in the field of programmers, simply
because they found things missing or not working the way they wanted it
to. Just as designers have been able to study and develop their
processes and methodologies for years, now software (/programming) has
become one of these processes and should be studied and developed the
same way and with the same understanding. Maybe that way designers will
learn to develop their own, really "free", processes and escape the
physical metaphors.


